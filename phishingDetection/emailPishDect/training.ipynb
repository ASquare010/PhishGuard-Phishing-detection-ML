{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from modules.FeatureExtractionEmail import FeatureExtractionEmail\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'dataset/features_extracted_V2.csv'  # Replace with your file path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging previous and new dataset for re-traning \n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "# give path to your new Dataset\n",
    "newDatasetPath = 'dataset/-------.csv'\n",
    "\n",
    "df = pd.read_csv(newDatasetPath)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    obj = FeatureExtractionEmail(row['email'])\n",
    "    df1 = pd.concat([df1, obj.df], ignore_index=True)\n",
    "    df1['label'] = row['label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(file_path, mode='a', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Separating out a sample for prediction\n",
    "predict_rows = df.sample(60, random_state=13)\n",
    "df = df.drop(predict_rows.index)\n",
    "\n",
    "# Separating features and label\n",
    "df_Y = df['label']\n",
    "df_X = df.drop('label', axis=1)\n",
    "\n",
    "# Preparing the training data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_X)\n",
    "df_X_scaled = scaler.transform(df_X)\n",
    "joblib.dump(scaler, 'model/scaler_model.joblib')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X_scaled, df_Y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Transform your predict_rows for prediction\n",
    "\n",
    "predict_features = predict_rows.drop('label', axis=1)\n",
    "predict_labels = predict_rows['label']\n",
    "\n",
    "# Scale the features of the 15 rows\n",
    "predict_features_scaled = scaler.transform(predict_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "best_precision_accuracy = 0\n",
    "best_config = None\n",
    "best_model = None\n",
    "\n",
    "base_learners_set1 = [('rf', RandomForestClassifier(criterion='entropy', max_features='sqrt', min_samples_leaf=1, min_samples_split=3, n_estimators=100)), \n",
    "                      ('mlp', MLPClassifier(max_iter=500, activation='relu', alpha=0.001, hidden_layer_sizes=(20,), learning_rate='adaptive', solver='adam')),\n",
    "                      ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=15, n_neighbors=20, p=1, weights='distance'))]\n",
    "\n",
    "base_learners_set2 = [('rf', RandomForestClassifier(criterion='entropy', max_features='sqrt', min_samples_leaf=1, min_samples_split=3, n_estimators=100)), \n",
    "                      ('mlp', MLPClassifier(max_iter=500, activation='relu', alpha=0.001, hidden_layer_sizes=(20,), learning_rate='adaptive', solver='adam')), \n",
    "                      ('svm', SVC(C=10, kernel='rbf', tol=0.001))]\n",
    "\n",
    "base_learners_set3 = [('rf', RandomForestClassifier(criterion='entropy', max_features='sqrt', min_samples_leaf=1, min_samples_split=3, n_estimators=100)),\n",
    "                      ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=15, n_neighbors=20, p=1, weights='distance')), \n",
    "                      ('svm', SVC(C=10, kernel='rbf', tol=0.001))]\n",
    "\n",
    "base_learners_set4 = [('mlp', MLPClassifier(max_iter=500, activation='relu', alpha=0.001, hidden_layer_sizes=(20,), learning_rate='adaptive', solver='adam')),\n",
    "                      ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=15, n_neighbors=20, p=1, weights='distance')), \n",
    "                      ('svm', SVC(C=10, kernel='rbf', tol=0.001))]\n",
    "\n",
    "base_learners = [base_learners_set1, base_learners_set2, base_learners_set3, base_learners_set4]\n",
    "\n",
    "for idx, base_learner_group in enumerate(base_learners):\n",
    "    meta_learner = LogisticRegression()\n",
    "    clf = StackingClassifier(estimators=base_learner_group, final_estimator=meta_learner)\n",
    "\n",
    "    # Train the model on the full training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set and evaluate\n",
    "    test_predictions = clf.predict(X_test)\n",
    "    precision = precision_score(y_test, test_predictions) * 100\n",
    "    accuracy = accuracy_score(y_test, test_predictions) * 100\n",
    "    combined_metric = (precision + accuracy) / 2  # You can adjust the combination based on your preference\n",
    "\n",
    "    print(f'Model Configuration {idx + 1} - Test Set Evaluation:')\n",
    "    print('Accuracy:', accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('F1 Score:', f1_score(y_test, test_predictions) * 100)\n",
    "    print('Recall:', recall_score(y_test, test_predictions) * 100)\n",
    "    print('ROC AUC:', roc_auc_score(y_test, test_predictions) * 100)\n",
    "    print('Confusion Matrix:', confusion_matrix(y_test, test_predictions))\n",
    "    print('Combined Metric:', combined_metric)\n",
    "    print('-----------------------------------------\\n')\n",
    "\n",
    "    # Predict on the 15 rows and compare with actual labels\n",
    "    predict_predictions = clf.predict(predict_features_scaled)\n",
    "    print('Prediction on 15 Rows:')\n",
    "    print('Predicted Labels:', predict_predictions)\n",
    "    print('Actual Labels:', predict_labels.values)\n",
    "    print('-----------------------------------------\\n')\n",
    "\n",
    "    # Save the best configuration and model based on the combined metric\n",
    "    if combined_metric > best_precision_accuracy:\n",
    "        best_precision_accuracy = combined_metric\n",
    "        best_config = base_learner_group\n",
    "        best_model = clf\n",
    "\n",
    "# Now, best_config contains the base learners configuration, and best_model contains the best StackingClassifier.\n",
    "print(f'The best model configuration is {best_config} with a combined metric of {best_precision_accuracy:.2f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the best model to a file (you can choose the serialization method based on your preferences)\n",
    "joblib.dump(best_model, 'model/best_model1.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
